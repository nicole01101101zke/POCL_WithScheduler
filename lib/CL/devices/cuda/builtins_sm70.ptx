//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30794723
// Cuda compilation tools, release 11.6, V11.6.55
// Based on NVVM 7.0.1
//

.version 7.6
.target sm_70
.address_size 64

	// .globl	pocl_add_i32
// _ZZ20pocl_sgemm_local_f32E2As has been demoted
// _ZZ20pocl_sgemm_local_f32E2Bs has been demoted

.visible .entry pocl_add_i32(
	.param .u64 pocl_add_i32_param_0,
	.param .u64 pocl_add_i32_param_1,
	.param .u64 pocl_add_i32_param_2
)
{
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [pocl_add_i32_param_0];
	ld.param.u64 	%rd2, [pocl_add_i32_param_1];
	ld.param.u64 	%rd3, [pocl_add_i32_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	mul.wide.u32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u32 	%r5, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.nc.u32 	%r6, [%rd9];
	add.s32 	%r7, %r6, %r5;
	add.s64 	%rd10, %rd4, %rd7;
	st.global.u32 	[%rd10], %r7;
	ret;

}
	// .globl	pocl_mul_i32
.visible .entry pocl_mul_i32(
	.param .u64 pocl_mul_i32_param_0,
	.param .u64 pocl_mul_i32_param_1,
	.param .u64 pocl_mul_i32_param_2
)
{
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [pocl_mul_i32_param_0];
	ld.param.u64 	%rd2, [pocl_mul_i32_param_1];
	ld.param.u64 	%rd3, [pocl_mul_i32_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	cvta.to.global.u64 	%rd5, %rd2;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	mul.wide.u32 	%rd7, %r4, 4;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.u32 	%r5, [%rd8];
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.nc.u32 	%r6, [%rd9];
	mul.lo.s32 	%r7, %r6, %r5;
	add.s64 	%rd10, %rd4, %rd7;
	st.global.u32 	[%rd10], %r7;
	ret;

}
	// .globl	pocl_dnn_conv2d_int8_relu
.visible .entry pocl_dnn_conv2d_int8_relu(
	.param .u64 pocl_dnn_conv2d_int8_relu_param_0,
	.param .u64 pocl_dnn_conv2d_int8_relu_param_1,
	.param .u64 pocl_dnn_conv2d_int8_relu_param_2,
	.param .u64 pocl_dnn_conv2d_int8_relu_param_3,
	.param .u64 pocl_dnn_conv2d_int8_relu_param_4,
	.param .u64 pocl_dnn_conv2d_int8_relu_param_5,
	.param .u64 pocl_dnn_conv2d_int8_relu_param_6,
	.param .u64 pocl_dnn_conv2d_int8_relu_param_7
)
{
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd1, [pocl_dnn_conv2d_int8_relu_param_3];
	ld.param.u64 	%rd2, [pocl_dnn_conv2d_int8_relu_param_4];
	ld.param.u64 	%rd3, [pocl_dnn_conv2d_int8_relu_param_5];
	ld.param.u64 	%rd4, [pocl_dnn_conv2d_int8_relu_param_6];
	ld.param.u64 	%rd5, [pocl_dnn_conv2d_int8_relu_param_7];
	cvta.to.global.u64 	%rd6, %rd4;
	cvta.to.global.u64 	%rd7, %rd3;
	cvta.to.global.u64 	%rd8, %rd5;
	cvta.to.global.u64 	%rd9, %rd2;
	cvta.to.global.u64 	%rd10, %rd1;
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	mul.wide.u32 	%rd11, %r4, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.u32 	%r5, [%rd12];
	add.s64 	%rd13, %rd9, %rd11;
	ld.global.nc.u32 	%r6, [%rd13];
	mul.lo.s32 	%r7, %r6, %r5;
	add.s64 	%rd14, %rd8, %rd11;
	st.global.u32 	[%rd14], %r7;
	add.s64 	%rd15, %rd7, %rd11;
	ld.global.nc.u32 	%r8, [%rd15];
	mul.lo.s32 	%r9, %r5, %r8;
	add.s64 	%rd16, %rd6, %rd11;
	st.global.u32 	[%rd16], %r9;
	ret;

}
	// .globl	pocl_sgemm_local_f32
.visible .entry pocl_sgemm_local_f32(
	.param .u64 pocl_sgemm_local_f32_param_0,
	.param .u64 pocl_sgemm_local_f32_param_1,
	.param .u64 pocl_sgemm_local_f32_param_2,
	.param .u32 pocl_sgemm_local_f32_param_3,
	.param .u32 pocl_sgemm_local_f32_param_4,
	.param .u32 pocl_sgemm_local_f32_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .f32 	%f<57>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<37>;
	// demoted variable
	.shared .align 4 .b8 _ZZ20pocl_sgemm_local_f32E2As[1024];
	// demoted variable
	.shared .align 4 .b8 _ZZ20pocl_sgemm_local_f32E2Bs[1024];

	ld.param.u64 	%rd14, [pocl_sgemm_local_f32_param_0];
	ld.param.u64 	%rd15, [pocl_sgemm_local_f32_param_1];
	ld.param.u64 	%rd16, [pocl_sgemm_local_f32_param_2];
	ld.param.u32 	%rd1, [pocl_sgemm_local_f32_param_4];
	ld.param.u32 	%r8, [pocl_sgemm_local_f32_param_5];
	mov.u32 	%r9, %ntid.y;
	mov.u32 	%r10, %ctaid.y;
	mov.u32 	%r1, %tid.y;
	mad.lo.s32 	%r11, %r10, %r9, %r1;
	cvt.u64.u32 	%rd2, %r11;
	mov.u32 	%r12, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r2, %tid.x;
	mad.lo.s32 	%r14, %r13, %r12, %r2;
	cvt.u64.u32 	%rd3, %r14;
	shr.u32 	%r3, %r8, 4;
	setp.eq.s32 	%p1, %r3, 0;
	mov.f32 	%f56, 0f00000000;
	@%p1 bra 	$L__BB3_3;

	cvt.u64.u32 	%rd18, %r8;
	shl.b32 	%r15, %r1, 6;
	mov.u32 	%r16, _ZZ20pocl_sgemm_local_f32E2As;
	add.s32 	%r6, %r16, %r15;
	shl.b32 	%r17, %r2, 2;
	add.s32 	%r4, %r6, %r17;
	mov.u32 	%r18, _ZZ20pocl_sgemm_local_f32E2Bs;
	add.s32 	%r19, %r18, %r15;
	add.s32 	%r5, %r19, %r17;
	add.s32 	%r7, %r18, %r17;
	mul.lo.s64 	%rd19, %rd18, %rd2;
	cvt.u64.u32 	%rd20, %r2;
	add.s64 	%rd21, %rd19, %rd20;
	cvta.to.global.u64 	%rd22, %rd14;
	shl.b64 	%rd23, %rd21, 2;
	add.s64 	%rd35, %rd22, %rd23;
	cvt.u64.u32 	%rd24, %r1;
	mul.lo.s64 	%rd25, %rd24, %rd1;
	add.s64 	%rd26, %rd25, %rd3;
	cvta.to.global.u64 	%rd27, %rd15;
	shl.b64 	%rd28, %rd26, 2;
	add.s64 	%rd34, %rd27, %rd28;
	shl.b64 	%rd6, %rd1, 6;
	cvt.u64.u32 	%rd7, %r3;
	mov.u64 	%rd36, 0;

$L__BB3_2:
	ld.global.nc.f32 	%f6, [%rd35];
	st.shared.f32 	[%r4], %f6;
	ld.global.nc.f32 	%f7, [%rd34];
	st.shared.f32 	[%r5], %f7;
	bar.sync 	0;
	ld.shared.f32 	%f8, [%r7];
	ld.shared.f32 	%f9, [%r6];
	fma.rn.f32 	%f10, %f9, %f8, %f56;
	ld.shared.f32 	%f11, [%r7+64];
	ld.shared.f32 	%f12, [%r6+4];
	fma.rn.f32 	%f13, %f12, %f11, %f10;
	ld.shared.f32 	%f14, [%r7+128];
	ld.shared.f32 	%f15, [%r6+8];
	fma.rn.f32 	%f16, %f15, %f14, %f13;
	ld.shared.f32 	%f17, [%r7+192];
	ld.shared.f32 	%f18, [%r6+12];
	fma.rn.f32 	%f19, %f18, %f17, %f16;
	ld.shared.f32 	%f20, [%r7+256];
	ld.shared.f32 	%f21, [%r6+16];
	fma.rn.f32 	%f22, %f21, %f20, %f19;
	ld.shared.f32 	%f23, [%r7+320];
	ld.shared.f32 	%f24, [%r6+20];
	fma.rn.f32 	%f25, %f24, %f23, %f22;
	ld.shared.f32 	%f26, [%r7+384];
	ld.shared.f32 	%f27, [%r6+24];
	fma.rn.f32 	%f28, %f27, %f26, %f25;
	ld.shared.f32 	%f29, [%r7+448];
	ld.shared.f32 	%f30, [%r6+28];
	fma.rn.f32 	%f31, %f30, %f29, %f28;
	ld.shared.f32 	%f32, [%r7+512];
	ld.shared.f32 	%f33, [%r6+32];
	fma.rn.f32 	%f34, %f33, %f32, %f31;
	ld.shared.f32 	%f35, [%r7+576];
	ld.shared.f32 	%f36, [%r6+36];
	fma.rn.f32 	%f37, %f36, %f35, %f34;
	ld.shared.f32 	%f38, [%r7+640];
	ld.shared.f32 	%f39, [%r6+40];
	fma.rn.f32 	%f40, %f39, %f38, %f37;
	ld.shared.f32 	%f41, [%r7+704];
	ld.shared.f32 	%f42, [%r6+44];
	fma.rn.f32 	%f43, %f42, %f41, %f40;
	ld.shared.f32 	%f44, [%r7+768];
	ld.shared.f32 	%f45, [%r6+48];
	fma.rn.f32 	%f46, %f45, %f44, %f43;
	ld.shared.f32 	%f47, [%r7+832];
	ld.shared.f32 	%f48, [%r6+52];
	fma.rn.f32 	%f49, %f48, %f47, %f46;
	ld.shared.f32 	%f50, [%r7+896];
	ld.shared.f32 	%f51, [%r6+56];
	fma.rn.f32 	%f52, %f51, %f50, %f49;
	ld.shared.f32 	%f53, [%r7+960];
	ld.shared.f32 	%f54, [%r6+60];
	fma.rn.f32 	%f56, %f54, %f53, %f52;
	bar.sync 	0;
	add.s64 	%rd35, %rd35, 64;
	add.s64 	%rd34, %rd34, %rd6;
	add.s64 	%rd36, %rd36, 1;
	setp.lt.u64 	%p2, %rd36, %rd7;
	@%p2 bra 	$L__BB3_2;

$L__BB3_3:
	mul.lo.s64 	%rd29, %rd1, %rd2;
	add.s64 	%rd30, %rd29, %rd3;
	cvta.to.global.u64 	%rd31, %rd16;
	shl.b64 	%rd32, %rd30, 2;
	add.s64 	%rd33, %rd31, %rd32;
	st.global.f32 	[%rd33], %f56;
	ret;

}
	// .globl	pocl_sgemm_scale_tensor_f16f16f32
.visible .entry pocl_sgemm_scale_tensor_f16f16f32(
	.param .u64 pocl_sgemm_scale_tensor_f16f16f32_param_0,
	.param .u64 pocl_sgemm_scale_tensor_f16f16f32_param_1,
	.param .u64 pocl_sgemm_scale_tensor_f16f16f32_param_2,
	.param .u32 pocl_sgemm_scale_tensor_f16f16f32_param_3,
	.param .u32 pocl_sgemm_scale_tensor_f16f16f32_param_4,
	.param .u32 pocl_sgemm_scale_tensor_f16f16f32_param_5,
	.param .f32 pocl_sgemm_scale_tensor_f16f16f32_param_6,
	.param .f32 pocl_sgemm_scale_tensor_f16f16f32_param_7
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<254>;
	.reg .b32 	%r<147>;
	.reg .b64 	%rd<33>;


	ld.param.u64 	%rd7, [pocl_sgemm_scale_tensor_f16f16f32_param_0];
	ld.param.u64 	%rd8, [pocl_sgemm_scale_tensor_f16f16f32_param_1];
	ld.param.u32 	%r18, [pocl_sgemm_scale_tensor_f16f16f32_param_3];
	ld.param.u32 	%r20, [pocl_sgemm_scale_tensor_f16f16f32_param_4];
	ld.param.u32 	%r19, [pocl_sgemm_scale_tensor_f16f16f32_param_5];
	cvta.to.global.u64 	%rd1, %rd7;
	mov.u32 	%r21, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r24, %r22, %r21, %r23;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, WARP_SZ;
	div.u32 	%r28, %r24, %r27;
	mov.u32 	%r29, %tid.y;
	mad.lo.s32 	%r30, %r26, %r25, %r29;
	cvta.to.global.u64 	%rd2, %rd8;
	shl.b32 	%r1, %r28, 4;
	shl.b32 	%r2, %r30, 4;
	setp.lt.u32 	%p2, %r1, %r18;
	setp.lt.u32 	%p3, %r2, %r20;
	and.pred  	%p1, %p3, %p2;
	setp.eq.s32 	%p4, %r19, 0;
	mov.f32 	%f189, 0f00000000;
	mov.f32 	%f188, %f189;
	mov.f32 	%f187, %f189;
	mov.f32 	%f186, %f189;
	mov.f32 	%f185, %f189;
	mov.f32 	%f184, %f189;
	mov.f32 	%f183, %f189;
	mov.f32 	%f182, %f189;
	@%p4 bra 	$L__BB4_17;

	mul.lo.s32 	%r3, %r2, %r19;
	add.s32 	%r32, %r19, -1;
	shr.u32 	%r33, %r32, 4;
	add.s32 	%r4, %r33, 1;
	and.b32  	%r5, %r4, 3;
	setp.lt.u32 	%p5, %r32, 48;
	mov.u32 	%r144, 0;
	mov.f32 	%f182, 0f00000000;
	mov.f32 	%f183, %f182;
	mov.f32 	%f184, %f182;
	mov.f32 	%f185, %f182;
	mov.f32 	%f186, %f182;
	mov.f32 	%f187, %f182;
	mov.f32 	%f188, %f182;
	mov.f32 	%f189, %f182;
	@%p5 bra 	$L__BB4_12;

	sub.s32 	%r143, %r4, %r5;
	not.pred 	%p6, %p1;

$L__BB4_3:
	@%p6 bra 	$L__BB4_5;

	mad.lo.s32 	%r35, %r144, %r18, %r1;
	mul.wide.s32 	%rd9, %r35, 2;
	add.s64 	%rd10, %rd1, %rd9;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r36, %r37, %r38, %r39, %r40, %r41, %r42, %r43}, [%rd10], %r18;
	add.s32 	%r44, %r144, %r3;
	mul.wide.s32 	%rd11, %r44, 2;
	add.s64 	%rd12, %rd2, %rd11;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r45, %r46, %r47, %r48, %r49, %r50, %r51, %r52}, [%rd12], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182}, {%r36, %r37, %r38, %r39, %r40, %r41, %r42, %r43}, {%r45, %r46, %r47, %r48, %r49, %r50, %r51, %r52}, {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182};

$L__BB4_5:
	@%p6 bra 	$L__BB4_7;

	add.s32 	%r53, %r144, 16;
	mad.lo.s32 	%r54, %r53, %r18, %r1;
	mul.wide.s32 	%rd13, %r54, 2;
	add.s64 	%rd14, %rd1, %rd13;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r55, %r56, %r57, %r58, %r59, %r60, %r61, %r62}, [%rd14], %r18;
	add.s32 	%r63, %r53, %r3;
	mul.wide.s32 	%rd15, %r63, 2;
	add.s64 	%rd16, %rd2, %rd15;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r64, %r65, %r66, %r67, %r68, %r69, %r70, %r71}, [%rd16], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182}, {%r55, %r56, %r57, %r58, %r59, %r60, %r61, %r62}, {%r64, %r65, %r66, %r67, %r68, %r69, %r70, %r71}, {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182};

$L__BB4_7:
	@%p6 bra 	$L__BB4_9;

	add.s32 	%r72, %r144, 32;
	mad.lo.s32 	%r73, %r72, %r18, %r1;
	mul.wide.s32 	%rd17, %r73, 2;
	add.s64 	%rd18, %rd1, %rd17;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r74, %r75, %r76, %r77, %r78, %r79, %r80, %r81}, [%rd18], %r18;
	add.s32 	%r82, %r72, %r3;
	mul.wide.s32 	%rd19, %r82, 2;
	add.s64 	%rd20, %rd2, %rd19;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r83, %r84, %r85, %r86, %r87, %r88, %r89, %r90}, [%rd20], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182}, {%r74, %r75, %r76, %r77, %r78, %r79, %r80, %r81}, {%r83, %r84, %r85, %r86, %r87, %r88, %r89, %r90}, {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182};

$L__BB4_9:
	@%p6 bra 	$L__BB4_11;

	add.s32 	%r91, %r144, 48;
	mad.lo.s32 	%r92, %r91, %r18, %r1;
	mul.wide.s32 	%rd21, %r92, 2;
	add.s64 	%rd22, %rd1, %rd21;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r93, %r94, %r95, %r96, %r97, %r98, %r99, %r100}, [%rd22], %r18;
	add.s32 	%r101, %r91, %r3;
	mul.wide.s32 	%rd23, %r101, 2;
	add.s64 	%rd24, %rd2, %rd23;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r102, %r103, %r104, %r105, %r106, %r107, %r108, %r109}, [%rd24], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182}, {%r93, %r94, %r95, %r96, %r97, %r98, %r99, %r100}, {%r102, %r103, %r104, %r105, %r106, %r107, %r108, %r109}, {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182};

$L__BB4_11:
	add.s32 	%r144, %r144, 64;
	add.s32 	%r143, %r143, -4;
	setp.ne.s32 	%p10, %r143, 0;
	@%p10 bra 	$L__BB4_3;

$L__BB4_12:
	add.s32 	%r137, %r19, -1;
	shr.u32 	%r136, %r137, 4;
	add.s32 	%r135, %r136, 1;
	and.b32  	%r134, %r135, 3;
	setp.eq.s32 	%p11, %r134, 0;
	@%p11 bra 	$L__BB4_17;

	add.s32 	%r141, %r19, -1;
	shr.u32 	%r140, %r141, 4;
	add.s32 	%r139, %r140, 1;
	and.b32  	%r146, %r139, 3;
	add.s32 	%r110, %r144, %r3;
	mul.wide.s32 	%rd25, %r110, 2;
	add.s64 	%rd32, %rd2, %rd25;
	mul.lo.s32 	%r145, %r144, %r18;
	shl.b32 	%r13, %r18, 4;
	not.pred 	%p12, %p1;

$L__BB4_14:
	.pragma "nounroll";
	@%p12 bra 	$L__BB4_16;

	add.s32 	%r111, %r145, %r1;
	mul.wide.s32 	%rd26, %r111, 2;
	add.s64 	%rd27, %rd1, %rd26;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r112, %r113, %r114, %r115, %r116, %r117, %r118, %r119}, [%rd27], %r18;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r120, %r121, %r122, %r123, %r124, %r125, %r126, %r127}, [%rd32], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182}, {%r112, %r113, %r114, %r115, %r116, %r117, %r118, %r119}, {%r120, %r121, %r122, %r123, %r124, %r125, %r126, %r127}, {%f189, %f188, %f187, %f186, %f185, %f184, %f183, %f182};

$L__BB4_16:
	add.s64 	%rd32, %rd32, 32;
	add.s32 	%r145, %r145, %r13;
	add.s32 	%r146, %r146, -1;
	setp.ne.s32 	%p13, %r146, 0;
	@%p13 bra 	$L__BB4_14;

$L__BB4_17:
	not.pred 	%p14, %p1;
	@%p14 bra 	$L__BB4_19;

	ld.param.f32 	%f173, [pocl_sgemm_scale_tensor_f16f16f32_param_6];
	ld.param.f32 	%f172, [pocl_sgemm_scale_tensor_f16f16f32_param_7];
	ld.param.u64 	%rd31, [pocl_sgemm_scale_tensor_f16f16f32_param_2];
	mov.u32 	%r133, %tid.y;
	mov.u32 	%r132, %ntid.y;
	mov.u32 	%r131, %ctaid.y;
	mad.lo.s32 	%r130, %r131, %r132, %r133;
	shl.b32 	%r129, %r130, 4;
	mad.lo.s32 	%r128, %r129, %r18, %r1;
	cvta.to.global.u64 	%rd28, %rd31;
	mul.wide.s32 	%rd29, %r128, 4;
	add.s64 	%rd30, %rd28, %rd29;
	wmma.load.c.sync.aligned.col.m16n16k16.global.f32 	{%f148, %f149, %f150, %f151, %f152, %f153, %f154, %f155}, [%rd30], %r18;
	mul.f32 	%f156, %f148, %f172;
	fma.rn.f32 	%f157, %f189, %f173, %f156;
	mul.f32 	%f158, %f149, %f172;
	fma.rn.f32 	%f159, %f188, %f173, %f158;
	mul.f32 	%f160, %f150, %f172;
	fma.rn.f32 	%f161, %f187, %f173, %f160;
	mul.f32 	%f162, %f151, %f172;
	fma.rn.f32 	%f163, %f186, %f173, %f162;
	mul.f32 	%f164, %f152, %f172;
	fma.rn.f32 	%f165, %f185, %f173, %f164;
	mul.f32 	%f166, %f153, %f172;
	fma.rn.f32 	%f167, %f184, %f173, %f166;
	mul.f32 	%f168, %f154, %f172;
	fma.rn.f32 	%f169, %f183, %f173, %f168;
	mul.f32 	%f170, %f155, %f172;
	fma.rn.f32 	%f171, %f182, %f173, %f170;
	wmma.store.d.sync.aligned.col.m16n16k16.global.f32 	[%rd30], {%f157, %f159, %f161, %f163, %f165, %f167, %f169, %f171}, %r18;

$L__BB4_19:
	ret;

}
	// .globl	pocl_sgemm_tensor_f16f16f32
.visible .entry pocl_sgemm_tensor_f16f16f32(
	.param .u64 pocl_sgemm_tensor_f16f16f32_param_0,
	.param .u64 pocl_sgemm_tensor_f16f16f32_param_1,
	.param .u64 pocl_sgemm_tensor_f16f16f32_param_2,
	.param .u32 pocl_sgemm_tensor_f16f16f32_param_3,
	.param .u32 pocl_sgemm_tensor_f16f16f32_param_4,
	.param .u32 pocl_sgemm_tensor_f16f16f32_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .f32 	%f<185>;
	.reg .b32 	%r<142>;
	.reg .b64 	%rd<32>;


	ld.param.u64 	%rd8, [pocl_sgemm_tensor_f16f16f32_param_0];
	ld.param.u64 	%rd9, [pocl_sgemm_tensor_f16f16f32_param_1];
	ld.param.u64 	%rd7, [pocl_sgemm_tensor_f16f16f32_param_2];
	ld.param.u32 	%r17, [pocl_sgemm_tensor_f16f16f32_param_3];
	ld.param.u32 	%r18, [pocl_sgemm_tensor_f16f16f32_param_4];
	ld.param.u32 	%r19, [pocl_sgemm_tensor_f16f16f32_param_5];
	cvta.to.global.u64 	%rd1, %rd9;
	cvta.to.global.u64 	%rd2, %rd8;
	setp.eq.s32 	%p2, %r19, 0;
	@%p2 bra 	$L__BB5_17;

	cvta.to.global.u64 	%rd10, %rd7;
	mov.u32 	%r21, WARP_SZ;
	mov.u32 	%r22, %tid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %ntid.x;
	mad.lo.s32 	%r25, %r23, %r24, %r22;
	div.u32 	%r26, %r25, %r21;
	shl.b32 	%r1, %r26, 4;
	setp.lt.u32 	%p3, %r1, %r17;
	mov.u32 	%r27, %tid.y;
	mov.u32 	%r28, %ctaid.y;
	mov.u32 	%r29, %ntid.y;
	mad.lo.s32 	%r30, %r28, %r29, %r27;
	shl.b32 	%r31, %r30, 4;
	setp.lt.u32 	%p4, %r31, %r18;
	and.pred  	%p1, %p4, %p3;
	mul.lo.s32 	%r2, %r31, %r19;
	mad.lo.s32 	%r32, %r31, %r17, %r1;
	mul.wide.s32 	%rd11, %r32, 4;
	add.s64 	%rd3, %rd10, %rd11;
	add.s32 	%r33, %r19, -1;
	shr.u32 	%r34, %r33, 4;
	add.s32 	%r3, %r34, 1;
	and.b32  	%r4, %r3, 3;
	setp.lt.u32 	%p5, %r33, 48;
	mov.u32 	%r139, 0;
	mov.f32 	%f129, 0f00000000;
	mov.f32 	%f130, %f129;
	mov.f32 	%f131, %f129;
	mov.f32 	%f132, %f129;
	mov.f32 	%f133, %f129;
	mov.f32 	%f134, %f129;
	mov.f32 	%f135, %f129;
	mov.f32 	%f136, %f129;
	@%p5 bra 	$L__BB5_12;

	sub.s32 	%r138, %r3, %r4;
	not.pred 	%p6, %p1;

$L__BB5_3:
	@%p6 bra 	$L__BB5_5;

	mad.lo.s32 	%r36, %r139, %r17, %r1;
	mul.wide.s32 	%rd12, %r36, 2;
	add.s64 	%rd13, %rd2, %rd12;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r37, %r38, %r39, %r40, %r41, %r42, %r43, %r44}, [%rd13], %r17;
	add.s32 	%r45, %r139, %r2;
	mul.wide.s32 	%rd14, %r45, 2;
	add.s64 	%rd15, %rd1, %rd14;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r46, %r47, %r48, %r49, %r50, %r51, %r52, %r53}, [%rd15], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, {%r37, %r38, %r39, %r40, %r41, %r42, %r43, %r44}, {%r46, %r47, %r48, %r49, %r50, %r51, %r52, %r53}, {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129};
	wmma.store.d.sync.aligned.col.m16n16k16.global.f32 	[%rd3], {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, %r17;

$L__BB5_5:
	@%p6 bra 	$L__BB5_7;

	add.s32 	%r54, %r139, 16;
	mad.lo.s32 	%r55, %r54, %r17, %r1;
	mul.wide.s32 	%rd16, %r55, 2;
	add.s64 	%rd17, %rd2, %rd16;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r56, %r57, %r58, %r59, %r60, %r61, %r62, %r63}, [%rd17], %r17;
	add.s32 	%r64, %r54, %r2;
	mul.wide.s32 	%rd18, %r64, 2;
	add.s64 	%rd19, %rd1, %rd18;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r65, %r66, %r67, %r68, %r69, %r70, %r71, %r72}, [%rd19], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, {%r56, %r57, %r58, %r59, %r60, %r61, %r62, %r63}, {%r65, %r66, %r67, %r68, %r69, %r70, %r71, %r72}, {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129};
	wmma.store.d.sync.aligned.col.m16n16k16.global.f32 	[%rd3], {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, %r17;

$L__BB5_7:
	@%p6 bra 	$L__BB5_9;

	add.s32 	%r73, %r139, 32;
	mad.lo.s32 	%r74, %r73, %r17, %r1;
	mul.wide.s32 	%rd20, %r74, 2;
	add.s64 	%rd21, %rd2, %rd20;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r75, %r76, %r77, %r78, %r79, %r80, %r81, %r82}, [%rd21], %r17;
	add.s32 	%r83, %r73, %r2;
	mul.wide.s32 	%rd22, %r83, 2;
	add.s64 	%rd23, %rd1, %rd22;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r84, %r85, %r86, %r87, %r88, %r89, %r90, %r91}, [%rd23], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, {%r75, %r76, %r77, %r78, %r79, %r80, %r81, %r82}, {%r84, %r85, %r86, %r87, %r88, %r89, %r90, %r91}, {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129};
	wmma.store.d.sync.aligned.col.m16n16k16.global.f32 	[%rd3], {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, %r17;

$L__BB5_9:
	@%p6 bra 	$L__BB5_11;

	add.s32 	%r92, %r139, 48;
	mad.lo.s32 	%r93, %r92, %r17, %r1;
	mul.wide.s32 	%rd24, %r93, 2;
	add.s64 	%rd25, %rd2, %rd24;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r94, %r95, %r96, %r97, %r98, %r99, %r100, %r101}, [%rd25], %r17;
	add.s32 	%r102, %r92, %r2;
	mul.wide.s32 	%rd26, %r102, 2;
	add.s64 	%rd27, %rd1, %rd26;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r103, %r104, %r105, %r106, %r107, %r108, %r109, %r110}, [%rd27], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, {%r94, %r95, %r96, %r97, %r98, %r99, %r100, %r101}, {%r103, %r104, %r105, %r106, %r107, %r108, %r109, %r110}, {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129};
	wmma.store.d.sync.aligned.col.m16n16k16.global.f32 	[%rd3], {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, %r17;

$L__BB5_11:
	add.s32 	%r139, %r139, 64;
	add.s32 	%r138, %r138, -4;
	setp.ne.s32 	%p10, %r138, 0;
	@%p10 bra 	$L__BB5_3;

$L__BB5_12:
	add.s32 	%r132, %r19, -1;
	shr.u32 	%r131, %r132, 4;
	add.s32 	%r130, %r131, 1;
	and.b32  	%r129, %r130, 3;
	setp.eq.s32 	%p11, %r129, 0;
	@%p11 bra 	$L__BB5_17;

	add.s32 	%r136, %r19, -1;
	shr.u32 	%r135, %r136, 4;
	add.s32 	%r134, %r135, 1;
	and.b32  	%r141, %r134, 3;
	add.s32 	%r111, %r139, %r2;
	mul.wide.s32 	%rd28, %r111, 2;
	add.s64 	%rd31, %rd1, %rd28;
	mul.lo.s32 	%r140, %r139, %r17;
	shl.b32 	%r12, %r17, 4;
	not.pred 	%p12, %p1;

$L__BB5_14:
	.pragma "nounroll";
	@%p12 bra 	$L__BB5_16;

	add.s32 	%r112, %r140, %r1;
	mul.wide.s32 	%rd29, %r112, 2;
	add.s64 	%rd30, %rd2, %rd29;
	wmma.load.a.sync.aligned.col.m16n16k16.global.f16 	{%r113, %r114, %r115, %r116, %r117, %r118, %r119, %r120}, [%rd30], %r17;
	wmma.load.b.sync.aligned.col.m16n16k16.global.f16 	{%r121, %r122, %r123, %r124, %r125, %r126, %r127, %r128}, [%rd31], %r19;
	wmma.mma.sync.aligned.col.col.m16n16k16.f32.f32 {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, {%r113, %r114, %r115, %r116, %r117, %r118, %r119, %r120}, {%r121, %r122, %r123, %r124, %r125, %r126, %r127, %r128}, {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129};
	wmma.store.d.sync.aligned.col.m16n16k16.global.f32 	[%rd3], {%f136, %f135, %f134, %f133, %f132, %f131, %f130, %f129}, %r17;

$L__BB5_16:
	add.s64 	%rd31, %rd31, 32;
	add.s32 	%r140, %r140, %r12;
	add.s32 	%r141, %r141, -1;
	setp.ne.s32 	%p13, %r141, 0;
	@%p13 bra 	$L__BB5_14;

$L__BB5_17:
	ret;

}

